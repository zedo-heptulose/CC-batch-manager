COMMANDS MUST EXECUTE IN JOB
WORKING DIRECTORY

SOLUTION:

subprocess cwd kwarg

for now, separating utilities
for restarting jobs from utils for 
queueing jobs

also should scancel 'failed' jobs just in carries

and why does it keep submitting failed jobs?

BUGS:
(addressed)
---nothing gets submitted after first pass,
because everything is error,error and not
not_started,not_started
---looks like finished finished isn't acknowledged
---failed jobs get repeatedly restarted
--perfectly fine jobs got killed (hasn't been replicated)
(this just... fixed itself?)

--jobs are randomly failing now?
--submits too many jobs. (was wrongly marking unstarted jobs as error finishes, but not killing them)


(to be addressed)
--doesn't kill failed jobs yet
--should kill error state jobs
to prevent unpredictable behavior


CAUSES:

--never ends if there's an 'error' present


ADDRESSING:
--doesn't kill failed jobs yet
--not very tolerant to out or slurm files already existing
*delete (or move) old slurm files when starting a job
*store the number of the (only) slurm output file after submitting 
*keep it in the ledger
...could also use this to query the scheduler for the status of the job, which is WAY more robust than what you're doing now

FIXED:
jobs now execute in proper directories
job not started issue is gone 

failed jobs get repeatedly resubmitted
(this seems to have fixed itself with the 'error','error' issue)




... how can I tell then?

BETTER SOLUTION:
store slurm id for each job in ledger
capture slurm status output to tell if success

CAP SOLUTION:
if completion success, you're done!
just check first. keep all else the same.

both seem to be addressed
let's try this

